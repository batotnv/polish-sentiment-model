{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import tensorflow as tf\n",
    "from livelossplot import PlotLossesKeras\n",
    "np.random.seed(7)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from gensim.models import Word2Vec, KeyedVectors, word2vec\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pickle\n",
    "import h5py\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#filename = 'C:/Users/Bartosz/OneDrive/magisterka semestr 3/UM/projekt nlp/ml-agh-course/data/polish_sentiment_dataset.csv'\n",
    "filename = 'C:/Users/Bartosz/OneDrive/magisterka semestr 3/UM/dane_nowe.csv'\n",
    "dataset = pd.read_csv(filename, delimiter = \";\")\n",
    "\n",
    "# Delete unused column\n",
    "#del dataset['length']\n",
    "\n",
    "# Delete All NaN values from columns=['description','rate']\n",
    "dataset = dataset[dataset['description'].notnull() & dataset['rate'].notnull()]\n",
    "\n",
    "# We set all strings as lower case letters\n",
    "#dataset['description'] = dataset['description'].str.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_names = [\"Neutral\",\"Positive\", \"Negative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.count())\n",
    "\n",
    "temp = dataset.groupby('rate').count()['description'].reset_index().sort_values(by='description',ascending=False)\n",
    "\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making dataset more symmetric\n",
    "\n",
    "dataset_negative = dataset[dataset['rate'] == -1.0]\n",
    "print(dataset_negative.count())\n",
    "dataset_positive = dataset[dataset['rate'] == 1.0]\n",
    "print(dataset_positive.count())\n",
    "dataset_neutral = dataset[dataset['rate'] == 0.0]\n",
    "print(dataset_neutral.count())\n",
    "\n",
    "dataset_positive_cut=dataset_positive[:183391]\n",
    "print(dataset_positive_cut.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##making dataset more symmetric\n",
    "\n",
    "print(dataset_positive_cut.count())\n",
    "frames = [dataset_positive_cut, dataset_negative, dataset_neutral]\n",
    "\n",
    "dataset = pd.concat(frames)\n",
    "print(dataset.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_path = 'C:/Users/Bartosz/OneDrive/magisterka semestr 3/UM/polishstopwords.txt'\n",
    "\n",
    "\n",
    "stop_words = pd.read_csv(stop_words_path, sep=\" \", header=None)\n",
    "stop_words.columns = [\"word\"]\n",
    "\n",
    "stop_words.head()\n",
    "\n",
    "stop_words = stop_words[\"word\"].tolist()\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    #lowecasre\n",
    "    text = str(text).lower()\n",
    "    #remove text in square brackets\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    #remove links\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    #remove punctuation and words containing numbers\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    #removing stop words\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)\n",
    "    \n",
    "\n",
    "test = \"hej, nie co tam i albo aczkolwiek, polska gola lewandowski\"\n",
    "\n",
    "print(clean_text(test))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text preprocessing\n",
    "\n",
    "dataset['description'] = dataset['description'].apply(lambda x:clean_text(x))\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "#from many_stop_words import get_stop_words\n",
    "\n",
    "#stop_words = get_stop_words('pl')\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "                    background_color='white',\n",
    "                    stopwords=stop_words,\n",
    "                    max_words=100,\n",
    "                    max_font_size=40,\n",
    "                    random_state=42\n",
    ").generate(str(dataset['description']))\n",
    "\n",
    "print(wordcloud)\n",
    "fig = plt.figure(1)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.savefig('Plots/woirdcloud.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "dataset['description'] = dataset['description'].apply(lambda x:str(x).split())\n",
    "top = Counter([item for sublist in dataset['description'] for item in sublist])\n",
    "temp = pd.DataFrame(top.most_common(20))\n",
    "temp = temp.iloc[1:,:]\n",
    "temp.columns = ['Common_words','count']\n",
    "temp.style.background_gradient(cmap='Purples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset['description']\n",
    "y = dataset['rate']\n",
    "\n",
    "#splitting dataset to train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#splitting train to train and validate\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)\n",
    "print(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dlugosc poszczegolnych zbiorow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape: \" + str(X_train.shape))\n",
    "print(\"X_test shape: \" + str(X_test.shape))\n",
    "print(\"X_val shape: \" + str(X_val.shape))\n",
    "print(\"y_train shape: \" + str(y_train.shape))\n",
    "print(\"y_test shape: \" + str(y_test.shape))\n",
    "print(\"y_val shape: \" + str(y_val.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stworzenie wektoru słów na bazie modelu z polskiej akademii nauk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('nkjp.txt', binary=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zamiana syn0 na vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_mean, emb_std = word2vec_model.vectors.mean(), word2vec_model.vectors.std()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = word2vec_model.vectors\n",
    "print('Shape of embedding matrix: ', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.most_similar(\"siema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize X_train and X_test to 2D tensor:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit_on_texts Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2 it is word -> index dictionary so every word gets a unique integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = embedding_matrix.shape[0]\n",
    "mxlen = 50\n",
    "nb_classes = 3\n",
    "\n",
    "tokenizer = Tokenizer(num_words=top_words)\n",
    "tokenizer.fit_on_texts(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "texts_to_sequences Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "sequences_val = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print(word_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pads sequences to the same length. Converts a class vector (integers) to binary class matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(sequences_train, maxlen=mxlen)\n",
    "X_test = sequence.pad_sequences(sequences_test, maxlen=mxlen)\n",
    "X_val = sequence.pad_sequences(sequences_val, maxlen=mxlen)\n",
    "\n",
    "#binary encoding\n",
    "y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "y_val = np_utils.to_categorical(y_val, nb_classes)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "\n",
    "\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"Neutral\",\"Positive\", \"Negative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating  LST network model (kind of RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "nb_epoch = 15\n",
    "\n",
    "embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                            embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(128, recurrent_dropout=0.3, dropout=0.3))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just need to compile the model and we will be ready to train it. When we compile the model, we declare the optimizer (Adam, SGD, etc.)  and the loss function. To fit the model, all we have to do is declare the number of epochs and the batch size.\n",
    "\n",
    "nb_epochs renamed to epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "rnn = model.fit(X_train, y_train, epochs= 15, batch_size=batch_size, shuffle=True, validation_data=(X_val, y_val))\n",
    "score = model.evaluate(X_val, y_val)\n",
    "print(\"Test Loss: %.2f%%\" % (score[0]*100))\n",
    "print(\"Test Accuracy: %.2f%%\" % (score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Save model...')\n",
    "model.save('Models/finalsentimentmodelv3.h5')\n",
    "print('Saved model to disk...')\n",
    "\n",
    "print('Save Word index...')\n",
    "output = open('Models/finalwordindexv3.pkl', 'wb')\n",
    "pickle.dump(word_index, output)\n",
    "print('Saved word index to disk...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rnn.history)\n",
    "pd.DataFrame(rnn.history).plot(figsize=(10, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "krzywe uczenia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.plot(rnn.history['accuracy'],'r')\n",
    "plt.plot(rnn.history['val_accuracy'],'g')\n",
    "plt.plot(rnn.history['loss'],'y')\n",
    "plt.plot(rnn.history['val_loss'],'b')\n",
    "plt.xticks(np.arange(0, nb_epoch+1, nb_epoch/5))\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training vs Validation Accuracy LSTM , epochs=15\") # for max length = 10 and 20 epochs\n",
    "plt.legend(['train_accuracy', 'val_accuracy','train_loss', 'val_loss'], loc=5, prop={'size': 14})\n",
    "\n",
    "# plt.figure(1)\n",
    "# plt.plot(rnn.history['loss'],'r')\n",
    "# plt.plot(rnn.history['val_loss'],'g')\n",
    "# plt.xticks(np.arange(0, nb_epoch+1, nb_epoch/5))\n",
    "# plt.rcParams['figure.figsize'] = (8, 6)\n",
    "# plt.xlabel(\"Num of Epochs\")\n",
    "# plt.ylabel(\"Training vs Validation Loss LSTM, epochs=15\") # for max length = 10 and 20 epochs\n",
    "# plt.legend(['train', 'validation'])\n",
    "plt.savefig('Plots/accuracy_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Precision-Recall\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert Y_Test into 1D array\n",
    "yy_true = [np.argmax(i) for i in y_test]\n",
    "print(yy_true)\n",
    "\n",
    "yy_scores = [np.argmax(i) for i in y_pred]\n",
    "print(yy_scores)\n",
    "\n",
    "print(\"Recall: \" + str(round(recall_score(yy_true, yy_scores, average='weighted'),2)))\n",
    "print(\"Precision: \" + str(round(precision_score(yy_true, yy_scores, average='weighted'),2)))\n",
    "print(\"F1 Score: \" + str(round(f1_score(yy_true, yy_scores, average='weighted'),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Confusion matrix\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_pred = model.predict(X_test, verbose=2)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "for ix in range(3):\n",
    "    print(ix, confusion_matrix(np.argmax(y_test, axis=1), y_pred)[ix].sum())\n",
    "cm = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Visualizing of confusion matrix\n",
    "import seaborn as sn\n",
    "\n",
    "df_cm = pd.DataFrame(cm, range(3), range(3))\n",
    "plt.figure(figsize=(10,7))\n",
    "sn.set(font_scale=1.4)\n",
    "sn.heatmap(df_cm, annot=True,fmt='d')\n",
    "sn.set_context(\"poster\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig('Plots/confusionMatrix.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras import models\n",
    "\n",
    "\n",
    "model = load_model('Models/finalsentimentmodelv3.h5')\n",
    "model.summary()\n",
    "\n",
    "with open('Models/finalwordindexv3.pkl', 'rb') as picklefile:\n",
    "    word_index = pickle.load(picklefile)\n",
    "top_words = len(word_index)\n",
    "tokenizer = Tokenizer(num_words=top_words)\n",
    "tokenizer.word_index = word_index\n",
    "print(word_index)\n",
    "\n",
    "\n",
    "print('Found %s uniqe tokens.' % len(word_index))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert text for example 'your sentence in Polish'\n",
    "text = ['ogólnie to jest bardzo beznadziejne pozdrawiam']\n",
    "\n",
    "\n",
    "text[0] = clean_text(text[0])\n",
    "print(text)\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "x_test = sequence.pad_sequences(test_sequences, maxlen=40)\n",
    "\n",
    "print('x_test shape:', str(x_test.shape))\n",
    "#model.load_weights('Models/finalsentimentmodel.h5')\n",
    "result = model.predict(x_test)\n",
    "print(result)\n",
    "print(\"Neutral: %.2f%%\" % (result[:,0]*100))\n",
    "print(\"Positive: %.2f%%\" % (result[:,1]*100))\n",
    "print(\"Negative: %.2f%%\" % (result[:,2]*100))\n",
    "#print(result)\n",
    "\n",
    "\n",
    "class_names[np.argmax(result)]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
